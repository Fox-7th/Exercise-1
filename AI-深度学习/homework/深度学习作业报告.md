# 深度学习作业报告

标签（空格分隔）： homework

---


> 前言： 远离高版本，时间比经验更宝贵！

# 1. ubuntu 16.04 torch7 + cuda/cudnn 环境配置

本文安装软件环境：ubuntu 16.04 + cuda 8.0

> 写在前面：之前因为电脑系统是ubuntu 16.10的缘故，经历了万般波折到最近也算是把环境配好了，但是过程中的痛苦自是不再想提及，由于系统版本的升级导致了一些依赖库版本的升级，而配置torch所需要的版本并不能与之兼容，所以有些库会强制进行降版本，我这里记录了一些，基于此，强烈不建议使用高版本。
```
bash install-deps

libjpeg-dev
liblcms2-dev
libwmf-dev
libx11-dev
libxext-dev
libxml2-dev
libfreetype6-dev
libtiff-dev libz-dev
libpng-dev
libbz2-dev
libltdl-dev

libltdl-dev -> libltdl7=2.4.6-0.1 ->
libbz2-dev -> libbz2-1.0=1.0.6-8 -> 
libpng-dev -> zlib1g-dev -> zlib1g=1:1.2.8.dfsg-2ubuntu4.1
libtiff-dev -> libjpeg-dev -> libjpeg8-dev -> libjpeg-turbo8-dev -> libjpeg-turbo8=1.4.2-0ubuntu3.1 
-> libtiff5=4.0.6-1ubuntu0.4 -> liblzma5=5.1.1alpha+20120614-2ubuntu2 ->liblzma-dev 
libfreetype6-dev -> libfreetype6=2.6.1-0.1ubuntu2.3
libxml2-dev -> libxml2=2.9.3+dfsg1-1ubuntu0.5
libxext-dev -> libx11-dev -> libx11-6 libx11-doc
libwmf-dev -> libwmf0.2-7
如上类推，后面带版本号，依次添加依赖


报没有 <readline>的错
sudo apt install libreadline-dev -> libreadline6-dev=6.3-8ubuntu2 -> libtinfo-dev
-> ruby-interpreter -> 预依赖 awk 使用 aptitude
```

## 1.1 安装torch

Torch是一个基于Lua语言的深度学习框架，官网地址[https://github.com/torch](https://github.com/torch)，安装torch7官方有文档，按照文档可完成[http://torch.ch/docs/getting-started.html#_](http://torch.ch/docs/getting-started.html#_)

安装之前首先确定你的Ubuntu下以安装git工具,通过Ctrl+Alt+T打开终端,在终端下输入git,加入终端输出，
![image_1cl93ve3q4vp1kjp1kfaq3j13992m.png-39kB][1]
则说明您的系统未安装git。在终端下输入sudo apt install git 即可安装,如下图所示。
![image_1cl940bnjeh71m4h2lr1ufe1gd433.png-692.1kB][2]
在确定成功安装git工具后就可以开始安装Torch7啦!

第一步:
获取安装LuaJIT(C语言编写的Lua的解释器)和Torch所必需的依赖包。代码如下:git clone https://github.com/torch/distro.git ~/torch --recursive      如下图所示。
![image_1cl9g20r61s5ar0jamd1lhv16q63g.png-1093.6kB][3]

第二步:
由于默认将依赖包下载在当前路径下的torch文件下,进入torch文件夹,打开并执行install-deps中的命令(这个命令执行时间可能有点长,别着急哈)。代码如下:

![image_1cl9gdt5kjtr12hj1c26jk71m403t.png-971.8kB][4]

第三步:
执行install.sh文件,如下图所示。
![image_1cl9h5hh4epmf2719pp1hs0bbf4a.png-976.9kB][5]

![image_1cl9h6kct1hjvlvmhifbkji424n.png-963.8kB][6]

终端提示:
Do you want to automatically prepend the Torch install location to PATH and LD_LIBRARY_PATH in your /home/guodongwei/.bashrc? (yes/no)
输入:yes     如上图所示。

第四步:
将路径手动添加到PATH变量中:
![image_1cl9h83f03c71gpp1gl31c1cdn254.png-19.9kB][7]
![image_1cl9h8kj21k317vv1ge91b6gllg5h.png-25.7kB][8]

第五步:
检测安装是否成功。在终端输入th命令,若出现下图,表明安装成功。
![image_1cl9h9g07ltb1abuo2115nsa9g5u.png-150.2kB][9]

## 1.2 安装hdf5

> Hierarchical Data Format(HDF)是一种针对大量数据进行组织和存储的文件格式。经历了20多年的发展，HDF格式的最新版本是HDF5，它包含了数据模型，库，和文件格式标准。以其便捷有效，移植性强，灵活可扩展的特点受到了广泛的关注和应用。

> 很多大型机构的数据存储格式都采用了HDF5，比如NASA的地球观测系统，MATLAB的.m文件，流体细算软件CDF，都将HDF5作为标准数据格式。现在HDF5还支持了大数据技术和NoSQL技术，并广泛用于科研，金融，以及其他科学和工程领域。

> HDF5在技术上提供了丰富的接口，包含C，C++，Fortran, Python, Java等，能够在不同的语言间完美兼容。

![hdf5的文件格式][10]

打开hdf5官方资源地址[hdf5](https://support.hdfgroup.org/HDF5/release/obtainsrc518.html)，下载源码如hdf5-1.8.18.tar.gz形式的文件：
```bash
$ tar -xzf hdf5-1.8.18.tar.gz
$ cd hdf5-1.8.18
$ ./configure --prefix=/usr/local/hdf5  #安装路径
$ make
$ make check # run test suite
$ install
$ make check-install # verify installation
```
这里我们介绍一个hdf5可视化工具，它可以用来查看和编辑HDF文件：HDFView，如图，
![image_1cl9iei8h1mtd44o1rdv1b8f1vi76o.png-51kB][11]

## 1.3 安装torch-hdf5

在github上[https://github.com/deepmind/torch-hdf5/blob/master/doc/usage.md](https://github.com/deepmind/torch-hdf5/blob/master/doc/usage.md)

```bash
$ sudo apt-get install libhdf5-serial-dev hdf5-tools
$ git clone https://github.com/deepmind/torch-hdf5
$ cd torch-hdf5
$ luarocks make hdf5-0-0.rockspec LIBHDF5_LIBDIR="/usr/lib/x86_64-linux-gnu/"
```

## 1.4 安装cuda8.0

### 1.4.1 检查计算机是否具备CUDA安装条件

```
$ lspci | grep -i nvidia
```
显示出GPU 版本：
![image_1clahc5v3kon12co1hhbl621bf075.png-14.8kB][12]

### 1.4.2 验证自己的Linux版本是否支持 CUDA

```
$ uname -m && cat /etc/*release
```
![image_1clahejs01sriaaf177c65neh7i.png-58.4kB][13]

### 1.4.3 安装驱动（apt方式）

```
$ sudo add-apt-repository ppa:graphics-drivers/ppa
$ sudo apt-get update
# 寻找合适的驱动版本
$ ubuntu-drivers devices
```
![image_1clahi0n51rveuffhp21vh7e617v.png-41.8kB][14]

```
$ sudo apt-get install nvidia-390
# 安装完重启
$ sudo reboot
# 查看驱动安装状态
$ sudo nvidia-smi
$ sudo nvidia-settings
```
![image_1clahkk101r716vo78713fbh7m8s.png-60.9kB][15]
![image_1clahl05r16u7127v1gjdvk5dog99.png-157.4kB][16]

### 1.4.4 安装cuda8.0

在官网下载cuda8.0 run文件，[https://developer.nvidia.com](https://developer.nvidia.com)，网站需要注册才可以下载；

![image_1clahpflaevi1sp41a331bre18sn9m.png-58.5kB][17]

```
# 关闭图形化界面
$ sudo service lightdm stop
# 安装cuda8.0
$ sudo sh cuda_8.0.44_linux.run
```
单击回车，直到提示“是否为NVIDIA安装驱动？” 
选择否，因为已经安装好驱动程序，其他都是默认。 

安装完成后可能还会需要添加这些库：

```
$ sudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev
# 完成后重启
$ sudo service lightdm start
```

最后需要设置环境变量：
```
$ sudo vim /etc/profile
```
在文件末尾添加以下两行：
```
$ export PATH=/usr/local/cuda-8.0/bin:$PATH
$ export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH
```
重启使环境变量生效！

## 1.5 安装cudnn，cudnn.torch

### 1.5.1 安装cudnn7

下载cudnn7所需要的包，[https://developer.nvidia.com/rdp/cudnn-download](https://developer.nvidia.com/rdp/cudnn-download)
![image_1clai9890c7iq27133716nh1pkra3.png-18.2kB][18]

按照如下方式安装：
```
# Navigate to your <cudnnpath> directory containing cuDNN Debian file.
# Install the runtime library, for example:
$ sudo dpkg -i libcudnn7_7.0.3.11-1+cuda9.0_amd64.deb
# Install the developer library, for example:
$ sudo dpkg -i libcudnn7-dev_7.0.3.11-1+cuda9.0_amd64.deb
# Install the code samples and the cuDNN Library User Guide, for example:
$ sudo dpkg -i libcudnn7-doc_7.0.3.11-1+cuda9.0_amd64.deb
```
安装完成后需要手动添加一下路径：

```
$ sudo cp /usr/local/cuda-8.0/lib64/libcudart.so.8.0 /usr/local/lib/libcudart.so.8.0 && sudo ldconfig
$ sudo cp /usr/local/cuda-8.0/lib64/libcublas.so.8.0 /usr/local/lib/libcublas.so.8.0 && sudo ldconfig
$ sudo cp /usr/local/cuda-8.0/lib64/libcurand.so.8.0 /usr/local/lib/libcurand.so.8.0 && sudo ldconfig
```

### 1.5.2 torch.cudnn

最后由于版本兼容性，我们还需要修改torch.cudnn的版本：
```
git clone https://github.com/soumith/cudnn.torch.git -b R7 && cd cudnn.torch && luarocks make cudnn-scm-1.rockspec
```
我们的torch就可以使用cudnn了！


## 1.6 安装lua-cjson

下载lua CJSON，[https://www.kyne.com.au/%7Emark/software/lua-cjson-manual.html](https://www.kyne.com.au/%7Emark/software/lua-cjson-manual.html)

将下载的包解压，按照如图方式添加：

![image_1clainn2c1vd31oonqgn7isk5nb0.png-23.8kB][19]

## 1.7 安装loadcaffe

添加loadcaffe库，[https://github.com/szagoruyko/loadcaffe](https://github.com/szagoruyko/loadcaffe)，如下所示：

```
# Install torch first. There is no Caffe dependency, only protobuf has to be installed. In Ubuntu do:
$ sudo apt-get install libprotobuf-dev protobuf-compiler
$ luarocks install loadcaffe
```

# 2.数据准备与实验说明

## 2.1 数据准备
为了对模型进行训练，我们对数据进行了前期的准备；由于所需要的数据绝大部分是由人工标注，工作量具大，我们团队经过不懈努力联系到同样进行类似实验的研究人员，经过沟通我们取得到了他们所用的所有实验数据，以下是工作人员邮箱，
![sli@ee.cuhk.edu.hk; xiaotong@ee.cuhk.edu.hk][20]

邮件原稿如下：
![image_1claj61ama4c1qbe1cmprrmmf8bq.png-52.2kB][21]

最后我们得到了如图的数据，

![image_1clak62nq1qsv1qkiffs1imgn6qdh.png-530.9kB][22]
![image_1clak5bhp1h3b1ufi3jj1r1d4ttd4.png-2803.2kB][23]
![image_1clajv3jb1a5n1t6b9gh12ok12rvc7.png-397.6kB][24]

## 2.2 实验说明
由于没有现有的数据集聚焦于描述人的自然语言现象，我们首先构建了一个大规模的语言数据集，从现有的人员重新识别数据集中，有40206张图片，13003人。亚马逊Mechanical Turk(AMT)上的两位独立工作者用两种不同的方式描述每个人的形象。在视觉方面，从不同的再识别数据集中汇集出来的人物图像在不同的场景、视图点和相机规格下，增加了图像内容的多样性。在语言方面，数据集有80412个句子描述，包含丰富的词汇、短语、句型和结构。 标签对描述人的局域网语言没有限制。我们对数据集进行了一系列的用户研究，以显示语言描述的丰富表达。

![image_1clalccq16hd13o57u7e6rm6o9.png-335.4kB][25]

视觉图像子网络使用人的图像数据矩阵 256×256 作为输入。与VGG-16网络具有相同的底层结构，在“drop7”层增加两个512单元的全连接层，产生512个视觉单元，v =$[v_1，…v_{512}]^\mathrm T$。我们的目标是联合训练整个网络，以确定在一张人像图片中，每个视觉单元决定是否存在某种外观模式。在我们的数据集上，视觉子网络首先对基于人物id的分类进行预训练。在与语言子网络的联合训练中，只更新两个新的全连通层(图5中的“cls-fc1”和“cls-fc2”)的参数，以便进行更有效的训练。注意，我们不会手动限制哪些单元学习什么概念。视觉单元的语义通过对整个网络的联合训练，自动捕获必要的语义概念。

为了有效地获取文字与图像之间的关系，该文提出了一种基于单元注意力机制的视觉单元模型（unit-level attention mechanism for visual units）——对于每个单词而言，具有与任一单词相似语义的视觉单元应被赋予更多权重。 以图5为例，给出“白色围巾”字样，语言子网络（language sub-network）将参与更多与“白色围巾”概念相对应的视觉单元模型。 我们训练语言子网络的目标正在于此。
语言子网络是一个LSTM网络，它能够有效地获取时序数据的时间关系。 给定输入句子，LSTM网络将会逐字逐句地产生视觉单元所需的单元注意力（unit-level attention）。 具体来说，这些单词将首先被编码为长度为K的one-hot向量，其中K是词汇表单词数量。给定描述性句子，可学习的全连接层将第t个原始单词转换为单词嵌入特征。 图片经过VGG-16的“drop7”层之后的两个具有512单元的全连接层后将得到LSTM的视觉特征。 在每步迭代下降中，LSTM将作为输入。
LSTM是由1个记忆单元和3个控制门组成的循环神经网络，分别是输入门，遗忘门，输出门。记忆单元负责保存对之前输入和现在输入的知识，于此同时，这些控制门又控制着信息的更新与流向。对于每个单词而言，LSTM是以下列方式更新记忆单元和输出隐藏层的:
![image_1clalhigsqivd1d2a3ot17gg13.png-29.8kB][26]
其中，代表元素互乘运算，W和b是需要训练和学习的参数。
为了使每个单词都能产生单元注意力，LSTM输出隐藏状态首先被送到具有ReLU非线性函数的全连接层，然后再经过具有softmax函数的全连接层，最终获得向量，其具有与视觉单元相同的维度。然后可以通过此获得句子与第个单词的人物图像之间的相似度：
![image_1clalkaqnrsui8j1dmu18fbjdn1g.png-8.5kB][27]
由于每个视觉单元确定图像中某些人物外观模式的存在，因此单独的视觉单元不能产生句子-图像相似度。由语言子网络生成的注意力大小决定应该将哪些视觉单元的响应相加以计算相似度。 如果语言子网络在某个视觉单元中产生高注意力，那么只有当视觉单元也具有高响应（表示某些视觉概念的存在）时，元素逐个相乘才会在该单词处产生高相似度值。 最终的句子-图像相似度是所有单词的注意力的总和，
![image_1clallbv51n891fqqlv81c62nfo1t.png-3kB][28]，
其中T是给定句子中的单词数。

单元级别的注意能够将最相关的单元与每个单词关联起来。然而，注意机制要求对不同单元的注意力相互竞争。在softmax非线性函数的例子中，我们有512个At(n)=1，并且发现这些约束对于学习有效注意非常重要。
然而，根据我们在2.2节中对不同词类型的用户研究，不同的词携带着显著的不同数量的信息以获得语言-图像的关联性。例如，“白色”这个词应该比“这个”更重要。在每个单词上，单位级别的关注度总和总是1，这并不能反映某种差异。因此，我们建议在每个单词上进行整体级别分等级的门控学习，从而对不同的单词进行加权。

单词等级的标量门是由一个将LSTM隐藏层状态ht经过全链接层后再通过sigmoid这种非线性函数 gt = σ(Wg ht + bg) 所得到的，其中σ 定义了sigmoid函数，wg和bg是全连接层的可学习的参数 。

单元级别的注意力和整体级别的门控都是每个单词在视觉单元上对每个单词及语言到图像关联性 $ a^t $ 的加权， 即：
$$ a_t = g_t\sum_{n=1}^{512}A_t(n)v_n $$
最终的关联性是以上所得所有关联的加权，$ a = \sum_{t=1}^T a_t $。

除了视觉子网络VGG-16的部分，本文提出的GNA-RNN进行了端到端随机梯度下降训练，它对人像分类进行了预训练和而后修正。从相应的句子-图像对作为正样本，非对应对作为负样本的数据集中随机选取训练样本。正样本与负样本之比为1:3。给定训练样本，训练的最小化交叉熵loss为，
$$ E=-\frac1N\sum^N_{i=1}[y^iloga^i + (1-y^i)log(1-a^i)]$$
$ a^i $表示第i个样本的预测关联度，$ y^i $表示？，1代表相应sentence-image对而0代表的不对应的句子-图像组。我们每组训练使用128个句子-图像对。除了字级门控以外的所有完全连接层都会有512个单元。

# 3. 源码分析

通过torch框架的cmdline模块，可以指定训练过程中的各项参数，包括最大迭代次数，输入数据路径等，具体如下。

![WX20180820-110053.png-90.3kB][29]
![image_1clakavo413st61112tgslh1l3sf8.png-3070.9kB][30]

项目代码中有两个关键子网络分别用于处理language和image。其中Image network使用了caffee格式的VGG16，使用loadcaffe包来进行解析加载，对应如下代码：
![image_1clakgoq1hld1qhnhvc314hnke.png-11.4kB][31]

Lanuage model基于nn包中module模块自行实现。nn是torch中用于实现各种神经网络计算的包，nn中的module是一个抽象类，定义了各种训练一个神经网络的基本方法，并且支持序列化，以便于保存训练出的模型。下面摘录部分源码作具体说明：
![image_1clakhoei1ddrqe510l4nfugc3l8.png-9.5kB][32]
![image_1claki0dmu1m12e8irgc081q1gll.png-14kB][33]

可以看到，代码定义了nn.LanguageModel，继承自nn.module。Layer：__init函数用于初始化language model, opt是用于初始化的参数，具体如下：
![image_1clakj6d9rgu1mb1kj61jgfcbmf.png-23.4kB][34]

参数中指定了用于batch trainning的大小，neural network的layer的数目，language tensor的size等信息。实现了两个关键子网络之后，要选择合适的Criterions。Criterions用于帮助训练神经网络，给定输入值和目标值之后，Criterions用来根据loss函数计算梯度。代码中选用了BCECriterion，即二值交叉熵，公式如下：
crossentropy(t,o) = -(t * log(o) + (1 - t) * log(1 - o))，对应代码如下：protos.crit = nn.BCECriterion() 。确定了language modle和image network，以及Criterions之后，还需要定义loss function，snippet如下：
![image_1clakk3bc4366jf1ehk120a1afjn9.png-77.3kB][35]

其中比较关键的有trainning，forward，和backward几个函数，其中training函数是自定义函数用于具体的训练逻辑，以language model为例子，代码如下：
![image_1clakksbmaad1t1ku12bebk27o3.png-28.5kB][36]

Forward和backward是继承自module的函数，函数签名分别为：[output] forward(input)，[gradInput] backward(input, gradOutput)。Backward即用来执行backpropagation这一步。
定义完模型的各个组件之后，就可以开始进行训练，也就进入了代码main loop之中，main loop 部分语义如下：

![image_1claklr6k1qls1eoh13igeen1is8pg.png-22.7kB][37]

可以看到当loss满足一定条件或到底指定迭代次数之后，训练停止。

# 4. 训练过程与结果

以下是训练过程截图：
![image_1clakvcf11k3v15uia062p11q0sn.png-574.7kB][38]
![image_1claktj9o1m1o1g7s3u414qgc5mqq.png-724.8kB][39]

以下是得出结果截图：
![image_1clakq2fa1atrakdj238jv18cdqd.png-2651.2kB][40]

# 5. 存在的问题与下一步改进的方向

## 5.1 存在的问题

1.CHUK-PEDES中的所有图片都经过了裁剪处理，这可能会丢失许多重要信息，比如：天气信息、地理信息、环境信息等，这些信息的缺失对于基于自然语言描述的人员搜索（Person Search）问题将产生致命影响。
2.GNA-RNN模型训练过程过于缓慢，我们研究小组两台Intel第七代i5加上GTX1060配置的计算机对模型训练过程实在是感到非常吃力，该模型严重依赖现有数据集，不知道在其他数据集进行测试的效果如何，特别是对于一些冷启动问题，该模型在解决时还感到无能为力。
3.严重质疑模型的泛化能力，我们由于时间有限，没有时间运行自己的数据来进行检验，但我们认真分析该模型，发现该模型由于结构设计的缺陷型，导致在处理类间差异的时候没有得到较好效果。
4.该模型直接使用了两个预训练的VGG16网络，作为提取图像特征的工具，这个未免过于草率，而且论文也没有对其进行超参数的检验，值得深思。

## 5.2 下一步改进的方向
1.根据end-to-end的思想，设计从视频截取的原生图像为图片库的新benchmark，从而让基于自然语言描述的算法更具有效益，更能帮助人们分析某个人行为特征。
2.重新选取其他图像特征提取网络，比如AlexNet、GoogLeNet、ResNet等，并进行相应的检测。


  [1]: http://static.zybuluo.com/usiege/0l5xmc5l8or6ue34q9c6gha8/image_1cl93ve3q4vp1kjp1kfaq3j13992m.png
  [2]: http://static.zybuluo.com/usiege/mehk90lbhefeg7ymwrgl8i6q/image_1cl940bnjeh71m4h2lr1ufe1gd433.png
  [3]: http://static.zybuluo.com/usiege/4dovt0h775ihn3dd9i58vfr9/image_1cl9g20r61s5ar0jamd1lhv16q63g.png
  [4]: http://static.zybuluo.com/usiege/adqjclocblfwlh6l94lp4168/image_1cl9gdt5kjtr12hj1c26jk71m403t.png
  [5]: http://static.zybuluo.com/usiege/ehgq5o7tnug41vmpwfamgdcn/image_1cl9h5hh4epmf2719pp1hs0bbf4a.png
  [6]: http://static.zybuluo.com/usiege/4rr97b6kifu313jr4z3lzvk0/image_1cl9h6kct1hjvlvmhifbkji424n.png
  [7]: http://static.zybuluo.com/usiege/0wlggj43zsihnn5kzcn5i888/image_1cl9h83f03c71gpp1gl31c1cdn254.png
  [8]: http://static.zybuluo.com/usiege/dwqzqucq02xp0ljh5jj9xtry/image_1cl9h8kj21k317vv1ge91b6gllg5h.png
  [9]: http://static.zybuluo.com/usiege/gpg17d9leaqd155s59b10kyr/image_1cl9h9g07ltb1abuo2115nsa9g5u.png
  [10]: http://static.zybuluo.com/usiege/v8bj8qf1tvbxw0w8uq0au0l1/image_1cl9hc7rh50t1kree9q1b8m1ed06b.png
  [11]: http://static.zybuluo.com/usiege/c6y8jy5nm52sxirzx3srkcmq/image_1cl9iei8h1mtd44o1rdv1b8f1vi76o.png
  [12]: http://static.zybuluo.com/usiege/bjkp1p6ejdku5paju7uu6aeu/image_1clahc5v3kon12co1hhbl621bf075.png
  [13]: http://static.zybuluo.com/usiege/pt0mhvfroyep26c6bn0q3lay/image_1clahejs01sriaaf177c65neh7i.png
  [14]: http://static.zybuluo.com/usiege/nup2zjoklyv6bizpiwuqc5yb/image_1clahi0n51rveuffhp21vh7e617v.png
  [15]: http://static.zybuluo.com/usiege/nvhd0b18vbh9ovdyo1ik02d4/image_1clahkk101r716vo78713fbh7m8s.png
  [16]: http://static.zybuluo.com/usiege/ck1k430ixzwghmzh9xwf9tyi/image_1clahl05r16u7127v1gjdvk5dog99.png
  [17]: http://static.zybuluo.com/usiege/nrske7w7i424fh36km45wp45/image_1clahpflaevi1sp41a331bre18sn9m.png
  [18]: http://static.zybuluo.com/usiege/609lqs916yfhm7gdwr193itf/image_1clai9890c7iq27133716nh1pkra3.png
  [19]: http://static.zybuluo.com/usiege/nrdk0w0t0arhcmoa91ngq1hn/image_1clainn2c1vd31oonqgn7isk5nb0.png
  [20]: http://static.zybuluo.com/usiege/24rtp1a3t3xznv0z7p94l9uy/image_1claj30us1spp16s67c61i4dtbd.png
  [21]: http://static.zybuluo.com/usiege/oklf1chennblhhna9gm4dqak/image_1claj61ama4c1qbe1cmprrmmf8bq.png
  [22]: http://static.zybuluo.com/usiege/cbdtlq9fpqxx33h8aw2xdrsy/image_1clak62nq1qsv1qkiffs1imgn6qdh.png
  [23]: http://static.zybuluo.com/usiege/71kj4f0qphqyyzmziqcx18bh/image_1clak5bhp1h3b1ufi3jj1r1d4ttd4.png
  [24]: http://static.zybuluo.com/usiege/k3p8uoxp1v5l7hfvc71qscnc/image_1clajv3jb1a5n1t6b9gh12ok12rvc7.png
  [25]: http://static.zybuluo.com/usiege/cksu3gburc7kjh421e3gx1gi/image_1clalccq16hd13o57u7e6rm6o9.png
  [26]: http://static.zybuluo.com/usiege/xf0gmcv27dengzd0aerjvyxg/image_1clalhigsqivd1d2a3ot17gg13.png
  [27]: http://static.zybuluo.com/usiege/tixl8bodf78k65pv62ydsgtu/image_1clalkaqnrsui8j1dmu18fbjdn1g.png
  [28]: http://static.zybuluo.com/usiege/szyzrye23zqj7vwi2jn0ojvt/image_1clallbv51n891fqqlv81c62nfo1t.png
  [29]: http://static.zybuluo.com/usiege/op2oq4xs6vd0caypuprsqnjq/WX20180820-110053.png
  [30]: http://static.zybuluo.com/usiege/yaf1a7554xsrbbikhuspcdmh/image_1clakavo413st61112tgslh1l3sf8.png
  [31]: http://static.zybuluo.com/usiege/xftlapglgls5yihxs27yyfpm/image_1clakgoq1hld1qhnhvc314hnke.png
  [32]: http://static.zybuluo.com/usiege/2osbi0o1umhlsl4q7hvr8vze/image_1clakhoei1ddrqe510l4nfugc3l8.png
  [33]: http://static.zybuluo.com/usiege/ye2awyhjg7dedz0crj4d4kdr/image_1claki0dmu1m12e8irgc081q1gll.png
  [34]: http://static.zybuluo.com/usiege/som5y3i9r2awnqunvodrlzqs/image_1clakj6d9rgu1mb1kj61jgfcbmf.png
  [35]: http://static.zybuluo.com/usiege/m9s196yqtjp476bed1a38jeo/image_1clakk3bc4366jf1ehk120a1afjn9.png
  [36]: http://static.zybuluo.com/usiege/d8rexdm0hl0jfml8u4o236n7/image_1clakksbmaad1t1ku12bebk27o3.png
  [37]: http://static.zybuluo.com/usiege/51ms1c1nngcg704fheo9y2n7/image_1claklr6k1qls1eoh13igeen1is8pg.png
  [38]: http://static.zybuluo.com/usiege/7jrvc2ih3nuuk5fs3dzz8doq/image_1clakvcf11k3v15uia062p11q0sn.png
  [39]: http://static.zybuluo.com/usiege/z23zjl7vqiob0747dc9opb5l/image_1claktj9o1m1o1g7s3u414qgc5mqq.png
  [40]: http://static.zybuluo.com/usiege/yvavan7zwsd123ryp2s2xigd/image_1clakq2fa1atrakdj238jv18cdqd.png